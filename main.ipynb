{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZFy81yd858i",
        "outputId": "31cd9b01-1015-4e69-a8a6-d16183adc149"
      },
      "outputs": [],
      "source": [
        "# Downloading the hate speech csv file from Google Drive\n",
        "!gdown 1oePItYlQYpQzG4FBreSFEIc7i8F662Vt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKmz2WI_jVs7",
        "outputId": "89e153c9-3538-43a1-df16-d5d87f903076"
      },
      "outputs": [],
      "source": [
        "# Downloading the bad words csv file from Google Drive\n",
        "!gdown 1ip1hr4trQ19S1ecC6Aea09PiTcxUhmlV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm-D71uQBMc6"
      },
      "outputs": [],
      "source": [
        "from csv import DictReader\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZV7b3YbACf_"
      },
      "outputs": [],
      "source": [
        "# Reading the dataset file with pandas\n",
        "hatefuldataset = pd.read_csv(\"labeled_tweet_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRjkkQ9Vtac-",
        "outputId": "21007619-75c1-42b2-bb94-671495020a17"
      },
      "outputs": [],
      "source": [
        "hatefuldataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIgfIkiMtmRy"
      },
      "outputs": [],
      "source": [
        "datasetFilter = hatefuldataset[['class','tweet']]\n",
        "class_label_value = (datasetFilter['class'].values).ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52X7Twq_uJa-",
        "outputId": "7b7eb153-cb03-494d-e918-6f75c9005fbd"
      },
      "outputs": [],
      "source": [
        "# Train and test data split\n",
        "dataTrain, dataTest = train_test_split(datasetFilter, test_size = 0.2, train_size = 0.8,  random_state = 30, stratify = datasetFilter['class'])\n",
        "\n",
        "# Shape of train and test data\n",
        "print(dataTrain.shape)\n",
        "print(dataTest.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXOiBMvCxbnh",
        "outputId": "88018084-8c85-40b7-f773-66e95997010c"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh6IvUZAyBvo"
      },
      "outputs": [],
      "source": [
        "# Stopwords that will be removed from strings\n",
        "finalStopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "finalStopwords.extend(['#ff', 'ff', 'rt'])\n",
        "finalStopwords.remove('not')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIMRq1OevZUI"
      },
      "outputs": [],
      "source": [
        "def preprocessTweet(tweetInfo):\n",
        "    ''' The function cleans up tweets by removing irrelevant information, derivational affixes and punctuation marks. '''\n",
        "    processedArr = []\n",
        "\n",
        "    # stemmer cuts off the ends of words (includes derivational affixes).\n",
        "    stemmer = PorterStemmer()\n",
        "    for each in tweetInfo:\n",
        "        # Removing mentions (starts with @)\n",
        "        process1 = re.sub(r'@[A-Za-z0-9_]+', ' ', each)\n",
        "\n",
        "        # Removing retweets\n",
        "        process2 = re.sub(r'RT', ' ', process1)\n",
        "\n",
        "        # Removing links\n",
        "        process3 = re.sub(r'https?', ' ', process2)\n",
        "        process4 = re.sub(r'https?://[A-Za-z0-9./]+', ' ', process3)\n",
        "\n",
        "        # Removing punctuations\n",
        "        process5 = re.sub(r'[^a-zA-Z]', ' ', process4)\n",
        "\n",
        "        # Removing hashtags (starts with #)\n",
        "        process6 = re.sub(r'&#[0-9]*',' ',process5)\n",
        "        process7 = re.sub(r'#[A-Za-z0-9]+',' ',process6)\n",
        "\n",
        "        # Removing image links\n",
        "        process7 = re.sub(r'pic.twitter.com/[A-Za-z0-9./]+',' ',process7)\n",
        "\n",
        "        # Turning every word to lowercase\n",
        "        process7 = process7.lower()\n",
        "        # Splitting words\n",
        "        process8 = process7.split()\n",
        "\n",
        "        # Stemming\n",
        "        process9 = [stemmer.stem(word) for word in process8 if not word in finalStopwords if len(word) > 2]\n",
        "\n",
        "        process9 = ' '.join(process9)\n",
        "        processedArr.append(process9)\n",
        "\n",
        "        finalProcess = np.array(processedArr)\n",
        "        \n",
        "    return finalProcess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm-YGenu8KC2"
      },
      "outputs": [],
      "source": [
        "def wordFrequencyDictionary(words):\n",
        "    ''' The function calculates how often a word is used. '''\n",
        "    freqDictionary = [(words.count(each))/len(words) for each in words]\n",
        "\n",
        "    return dict(list(zip(words,freqDictionary)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAchAzN28LnZ"
      },
      "outputs": [],
      "source": [
        "def sortDictionary(freqDictionary):\n",
        "    ''' The function sorts the frequnecy of words dictionary. '''\n",
        "    finalVersion = [(freqDictionary[each], each) for each in freqDictionary]\n",
        "    \n",
        "    finalVersion.sort()\n",
        "    finalVersion.reverse()\n",
        "    \n",
        "    return finalVersion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfK98m52-gN7"
      },
      "outputs": [],
      "source": [
        "def findCommonWord(words, x):\n",
        "    ''' The function finds common words that are used. '''\n",
        "    return ([each[1] for each in words])[:x]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pgygpXHxgF-"
      },
      "outputs": [],
      "source": [
        "processedArr = preprocessTweet(dataTrain['tweet'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfwoY3odzHtw",
        "outputId": "e3a952d2-d735-45b2-eead-8d136f74f608"
      },
      "outputs": [],
      "source": [
        "train2, valid2, train2_y, valid2_y = train_test_split(processedArr, dataTrain['class'], test_size = 0.2, random_state = 0, stratify=dataTrain['class'])\n",
        "\n",
        "print(train2.shape)\n",
        "print(valid2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpLi5w2q4Vko"
      },
      "outputs": [],
      "source": [
        "hatefulTweets = [sentence for sentence, label in zip(train2, class_label_value) if label == 0]\n",
        "hatefulWords = ' '.join(hatefulTweets)\n",
        "hatefulWords = wordFrequencyDictionary(hatefulWords.split())\n",
        "hatefulDictionary = sortDictionary(hatefulWords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNxNyc0w4XOv"
      },
      "outputs": [],
      "source": [
        "offensiveTweets = [sentence for sentence, label in zip(train2, class_label_value) if label == 1]\n",
        "offensiveWords = ' '.join(offensiveTweets)\n",
        "offensiveWords = wordFrequencyDictionary(offensiveWords.split())\n",
        "offensiveDictionary = sortDictionary(offensiveWords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2t-29_xfZDJ"
      },
      "outputs": [],
      "source": [
        "neutralTweets = [sentence for sentence, label in zip(train2, class_label_value) if label == 2]\n",
        "neutralWords = ' '.join(neutralTweets)\n",
        "neutralWords = wordFrequencyDictionary(neutralWords.split())\n",
        "neutralDictionary = sortDictionary(neutralWords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72-oufCukTWW"
      },
      "outputs": [],
      "source": [
        "common = list()\n",
        "\n",
        "common.append(findCommonWord(hatefulDictionary, 4000))\n",
        "common.append(findCommonWord(offensiveDictionary, 2000))\n",
        "common.append(findCommonWord(neutralDictionary, 2000))\n",
        "\n",
        "common = np.unique(np.hstack(common))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNY0MVTc-hs-"
      },
      "outputs": [],
      "source": [
        "commonDictionary = ({i:j for i, j in zip(common, range(len(common)))})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PYflM-_1eXp"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vangFtT8BKH",
        "outputId": "bc2fbe25-d43f-490f-f9cb-8c9cbd5e3678"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "newTokenizer = TweetTokenizer() \n",
        "newVectorizer = CountVectorizer(analyzer = \"word\", vocabulary = commonDictionary, tokenizer = newTokenizer.tokenize)\n",
        "\n",
        "train3 = newVectorizer.fit_transform(train2).toarray()\n",
        "valid3 = newVectorizer.transform(valid2).toarray()\n",
        "\n",
        "print(train3.shape)\n",
        "print(valid3.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNebR-fB2bHp"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajpCDNX6-mtN",
        "outputId": "b8f39483-ed2c-4104-b5b5-a697955ba8a6"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "startTime = time.time()\n",
        "\n",
        "modelLR = LogisticRegression(max_iter = 400, random_state = 0)\n",
        "modelLR.fit(train3, train2_y.ravel())\n",
        "prediction = modelLR.predict(valid3)\n",
        "\n",
        "endTime = time.time()\n",
        "print('Time passed: ' + str(endTime - startTime) + '\\n')\n",
        "\n",
        "print(classification_report(valid2_y, prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "S7jxj4cnvaud",
        "outputId": "948daebc-872a-40ed-ca8c-d4495b290d07"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "confusionMatrix = confusion_matrix(valid2_y, prediction)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix = confusionMatrix)\n",
        "disp.plot()\n",
        "\n",
        "plt.title('Logistic Regression')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duhFow7O2hsp"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRTCagYrsrVh",
        "outputId": "e27ce52b-bf4a-45a2-fe9c-6caec93158af"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "startTime = time.time()\n",
        "\n",
        "desTreeModel = DecisionTreeClassifier(random_state=0)\n",
        "desTreeModelFinal = desTreeModel.fit(train3, train2_y.ravel())\n",
        "prediction2 = desTreeModelFinal.predict(valid3)\n",
        "\n",
        "endTime = time.time()\n",
        "print('Time passed: ' + str(endTime - startTime) + '\\n')\n",
        "\n",
        "print(classification_report(valid2_y, prediction2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "njd5-UMmz9dX",
        "outputId": "bec650ca-043a-4d51-b7c6-e7f0b79b5d38"
      },
      "outputs": [],
      "source": [
        "confusionMatrix2 = confusion_matrix(valid2_y, prediction2)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix = confusionMatrix2)\n",
        "disp.plot()\n",
        "\n",
        "plt.title('Decision Tree')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJtW0f9I9H5w",
        "outputId": "09490d56-9fd9-476c-edf4-b54753e2ec32"
      },
      "outputs": [],
      "source": [
        "print('First Example')\n",
        "testList = preprocessTweet(datasetFilter['tweet'][180:181].values)\n",
        "X_test = newVectorizer.transform(testList).toarray()\n",
        "\n",
        "predictExample = modelLR.predict(X_test)\n",
        "print('Predicted Label: ' + str(predictExample[0]))\n",
        "\n",
        "actual = datasetFilter['class'][180:181].values\n",
        "print('Actual Label: ' + str(actual[0]))\n",
        "\n",
        "\n",
        "print('\\nSecond Example')\n",
        "testList2 = preprocessTweet(datasetFilter['tweet'][10:11].values)\n",
        "X_test2 = newVectorizer.transform(testList2).toarray()\n",
        "\n",
        "predictExample = modelLR.predict(X_test2)\n",
        "print('Predicted Label: ' + str(predictExample[0]))\n",
        "\n",
        "actual = datasetFilter['class'][10:11].values\n",
        "print('Actual Label: ' + str(actual[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMWHU1nJSp4x",
        "outputId": "48456e65-81c0-46d4-dae5-b0ac924f7a7f"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "  userInput = input(\"Enter the sentence you want to check: (If you want to quit, enter \\'quit\\')\")\n",
        "  if userInput == 'quit':\n",
        "    print()\n",
        "    break\n",
        "  userInput = [userInput]\n",
        "\n",
        "  userInputFinal = preprocessTweet(userInput)\n",
        "  user_test = newVectorizer.transform(userInputFinal).toarray()\n",
        "\n",
        "  user_predict = modelLR.predict(user_test)\n",
        "\n",
        "  arr2 = userInput[0].split()\n",
        "\n",
        "  a = ''\n",
        "\n",
        "  if user_predict[0] == 2:\n",
        "    for i in arr2:\n",
        "      x = False\n",
        "      with open('bad-words.csv', 'r') as wordColumn:\n",
        "        csvReadVar = DictReader(wordColumn)\n",
        "        for row in csvReadVar:\n",
        "          if str(row['bad_word_column']) == (i):\n",
        "            x = True\n",
        "      \n",
        "        if x == True:\n",
        "          a = 'The sentence does not contain any hate speech or offensive word but, it contains profane word\\n'\n",
        "        elif x == False:\n",
        "          a = 'The sentence does not contain any hate speech, offensive word or profane word\\n'\n",
        "\n",
        "  elif user_predict[0] == 1:\n",
        "      for i in arr2:\n",
        "        x = False\n",
        "        with open('bad-words.csv', 'r') as wordColumn:\n",
        "          csvReadVar = DictReader(wordColumn)\n",
        "          for row in csvReadVar:\n",
        "            if str(row['bad_word_column']) == (i):\n",
        "              x = True\n",
        "      \n",
        "          if x == True:\n",
        "            a = 'The sentence contains offensive word and profane word\\n'\n",
        "          elif x == False:\n",
        "            a = 'The sentence contains offensive word but, does not contains profane word\\n'\n",
        "\n",
        "  elif user_predict[0] == 0:\n",
        "        for i in arr2:\n",
        "          x = False\n",
        "          with open('bad-words.csv', 'r') as wordColumn:\n",
        "            csvReadVar = DictReader(wordColumn)\n",
        "            for row in csvReadVar:\n",
        "              if str(row['bad_word_column']) == (i):\n",
        "                x = True\n",
        "      \n",
        "          if x == True:\n",
        "            a = 'The sentence contains hate speech and profane word\\n'\n",
        "          elif x == False:\n",
        "            a = 'The sentence contains hate speech but, does not contains a profane word\\n'\n",
        "\n",
        "  print(a)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Artificial Intelligence Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
